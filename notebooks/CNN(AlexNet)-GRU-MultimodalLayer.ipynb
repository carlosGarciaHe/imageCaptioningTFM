{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CNN-GRU-MultimodalLayer\n\nThis notebook aims the construction, training and test of a CNN-LSTM model purposed in Vinyals et\nal. [15].\n\nThe CNN-LSTM model is a encoder-decoder designed for image caption generation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Setup of libraries"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "In the next cell we link the GPU hardware to tensorflow for use this component in the training process."
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": "import csv # To read the captions of a csv\nimport tensorflow as tf # To build and train the ANN\nfrom tensorflow.keras.preprocessing.text import Tokenizer # To use the tokenizer to split the words\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences # \nimport numpy as np\nimport pickle\nimport pandas as pd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Lectura de los datos"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Ahora leemos del CSV las descripciones y los id de las imagenes asociadas a el, para con estos formar el path de cada imagen."
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": "def idToPath(id_image):\n    id_image = str(id_image)\n    complete_name=id_image+\".jpg\"\n    while len(complete_name)<16:\n        complete_name=\"0\"+complete_name\n    return \"data/train2017/\"+complete_name"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": "PATH = \"data/train_machine_spanish.xlsx\"\ndf = pd.read_excel(PATH, names=[\"id_image\",\"caption\"])\ndf['caption'] = df.apply(lambda x: \"smark \"+x['caption']+\" emark\", axis=1)\ndf['id_image'] = df.apply(lambda x: idToPath(x['id_image']),axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": "PATH = \"data/validation.xlsx\"\nval_df = pd.read_excel(PATH, names=[\"id_image\",\"caption\"])\nval_df['caption'] = val_df.apply(lambda x: \"smark \"+x['caption']+\" emark\", axis=1)\nval_df['id_image'] = val_df.apply(lambda x: idToPath(x['id_image']),axis=1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Setup of text data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We load the tokenizer that we initialized in word embeddings notebook."
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": "# loading\nwith open('items/tokenizer_spanish.pkl', 'rb') as handle:\n    tokenizer = pickle.load(handle)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We transform the descriptions into a list of integers that represent the index in the tokenizer's word index vocabulary."
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": "sentences_x = tokenizer.texts_to_sequences(df['caption'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "And its equivalent for the labels that are the same lists with each element moved one position to the left"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": "sentences_y = [sentence[1:] for sentence in sentences_x]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We put pads on the right for every sentence that has less than 15 words."
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": "pad_sentences_x = pad_sequences(sentences_x, padding='post',maxlen=15)\npad_sentences_y = pad_sequences(sentences_y, padding='post',maxlen=15)"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([   2,    5,   55,    7,    1,  279,  230,    9, 2988,  450,    8,\n        139,    3,    0,    0], dtype=int32)"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "pad_sentences_x[0]"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([   5,   55,    7,    1,  279,  230,    9, 2988,  450,    8,  139,\n          3,    0,    0,    0], dtype=int32)"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "pad_sentences_y[0]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Model building"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The embedding layer loaded from the model stored in word embeddings notebook. This model transform word indexes to embeddings for encoder inputs."
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": "class Embeddings_Model(tf.keras.Model):\n    \n    def __init__(self, max_length, embedding_dimension):\n        super(Embeddings_Model, self).__init__()\n        weights = None\n        # Load the weight of the layer embedding pre-trained\n        with open('items/embeddingLayerWeights_spanish.pkl', 'rb') as handle:\n            weights = pickle.load(handle)\n        self.embedding = tf.keras.layers.Embedding(max_length+1, embedding_dimension ,weights=[weights])\n        \n    def call(self, inputs):\n        x = self.embedding(inputs)\n        \n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We define the enconder that is the InceptionV3 model. Its inputs are the images and the output is the image embedding."
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": "class CNN_Model(tf.keras.Model):\n    \n    def __init__(self):\n        super(CNN_Model, self).__init__()\n        # Load the InceptionV3 pre-trained model\n        self.input_model = tf.keras.applications.InceptionV3(include_top=True, weights='imagenet',classes=1000)\n        \n    def call(self, inputs):\n        x = self.input_model(inputs)\n        \n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We define the decoder that is composed by:\n\n- Embedding model: Transform word indexes to embeddings\n- GRU module: Generate step by step the words.\n- Dense layer: Transform LSTM outputs to one hot vector (without softmax)"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": "class GRU_Model(tf.keras.Model):\n    \n    def __init__(self, max_length, embedding_dimension, num_result_words):\n        super(GRU_Model, self).__init__()\n        # Dimension embedding number\n        self.embedding_dimension = embedding_dimension\n        # Output words number\n        self.num_result_words = num_result_words\n        # Layer to map from image embedding to lstm dimension\n        self.dense = tf.keras.layers.Dense(embedding_dimension)\n        # LSTM layer with 0.1 dropout\n        self.gru = tf.keras.layers.GRU(embedding_dimension, input_shape=(num_result_words, embedding_dimension),\n                                         return_sequences=True)\n        # Concatenate layer for merge image embeddings and gru output in the multimodal layer\n        self.concat = tf.keras.layers.Concatenate()\n        # The multimodal layer is similar to the softmax size but with a linear function\n        # because the softmax function is used in the loss function\n        self.output_model = tf.keras.layers.Dense(max_length, activation='linear')\n        \n    def call(self, inputs):\n        # input = (captions, initial_state)\n        captions, initial_state = inputs[0],inputs[1]\n        initial_state = self.dense(initial_state)\n        initial_state = tf.reshape(initial_state,[-1,self.embedding_dimension])\n        x = self.gru(captions, initial_state=initial_state)\n        # List of image embeddings used as one of the inputs to the multimodal layer\n        image_embedding = []\n        for i in range(x.shape[1]):\n            image_embedding.append(initial_state)\n        image_embedding = tf.reshape(image_embedding,[-1,x.shape[1],self.embedding_dimension])\n        x = self.concat([x,image_embedding])\n        x = self.output_model(x)\n        \n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We combine the encoder and decoder models in a unique model."
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": "class CNN_GRU_Model():\n    \n    # This function declare every layer of the model\n    def __init__(self):        \n        # Number of differents words in the tokenizer vocabulary.\n        self.max_length = 14276\n        # Embedding dimension\n        self.embedding_dimension = 512\n        # Maximum number or words that the model is able to generate in a caption.\n        self.num_result_words = 15\n        # Optimizer used\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n        # Initialize the encoder\n        self.encoder = CNN_Model()\n        # Initialize the embedding layer\n        self.embedding = Embeddings_Model(self.max_length, self.embedding_dimension)\n        # Initialize the decoder\n        self.decoder = GRU_Model(self.max_length, self.embedding_dimension, self.num_result_words)\n    \n    # This function load all images for the encoder can transform them to image embeddings\n    # Parameters:\n    #            image_paths: Collection of path to images\n    #            load: Boolean that indicate if load all images or read a file with the embeddings\n    # Return:\n    #        A list with the image embeddings\n    def encoder_predict(self, image_paths, load=False, test=False):\n        predictions = []\n        contador=0\n        if load or test:\n            # If load=True then we need to predict all image embeddings with the encoder\n            # and stored for future uses\n            for image_path in image_paths:\n                # Visual indicator for longs process\n                if contador%1000==0:\n                    print(\"Procesando imagen\",contador)\n                contador+=1\n                # Read the file with tensorflow function\n                image = tf.io.read_file(image_path)\n                # Transform the image to jpeg in RGB color space\n                image = tf.image.decode_jpeg(image, channels=3)\n                # Resize to inception_v3 input size\n                image = tf.image.resize(image, (299, 299))\n                # Normalize and other transforms\n                image = tf.keras.applications.inception_v3.preprocess_input(image)\n                # Add a dimension to tensor for the model\n                image = np.expand_dims(image, axis=0)\n                # Add the result of the prediction to the list\n                predictions.append(self.encoder.predict(image))\n            # Store the image embeddings list\n            if not test and len(predictions)>10000:\n                with open('items/image_embeddings_list.pkl', 'wb') as handle:\n                    pickle.dump(predictions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        else:\n            # Load the image embeddings list\n            with open('items/image_embeddings_list.pkl', 'rb') as handle:\n                predictions = pickle.load(handle)[:len(image_paths)]\n        \n        return predictions\n    \n    # This function preprocess the data\n    # Parameters:\n    #           X_image: Image path list\n    #           X_caption: Caption list\n    #           Y_caption: Label caption list for compare with the predict caption\n    def preprocess_data(self,X_image,X_caption,Y_caption, test=False):\n        # Get the image embeddings\n        image_embeddings = np.array(self.encoder_predict(X_image, test))\n        \n        # Lambda function for map the word embeddings\n        map_embeddings = lambda x: self.embedding.predict(np.array([x]))\n        # Map the word embeddings\n        X_caption_embeddings = map_embeddings(X_caption)\n        \n        # A set of transforms in numpy array sizes\n        Y_caption_embeddings = np.array(Y_caption)\n        X_caption_embeddings = X_caption_embeddings.reshape((\n            -1, self.num_result_words, self.embedding_dimension))\n        Y_caption_embeddings = Y_caption_embeddings.reshape((\n            -1, self.num_result_words))\n        \n        return image_embeddings, X_caption_embeddings, Y_caption_embeddings\n    \n    # This function train the model\n    # Parameters:\n    #           X_image: Image path list\n    #           X_caption: Caption list\n    #           Y_caption: Label caption list for compare with the predict caption\n    def train(self, X_image, X_caption, Y_caption):\n        \n        # Reshape of the data\n        image_embeddings, X_caption_embeddings, Y_caption_embeddings = self.preprocess_data(\n            X_image,\n            X_caption,\n            Y_caption,\n            True)\n#         val_image_embeddings, val_X_caption_embeddings, val_Y_caption_embeddings = self.preprocess_data(\n#             val_X_image,\n#             val_X_caption,\n#             val_Y_caption,\n#             True)\n        \n        # Initialize optimizer\n        optimizer = tf.keras.optimizers.Adam()\n        print(\"Image embeddings:\",image_embeddings.shape)\n        print(\"X_caption_embeddings\",X_caption_embeddings.shape)\n        print(\"Y_caption_embeddings\",Y_caption_embeddings.shape)\n        # Compile the model with sparse_cross_entropy as loss function\n        self.decoder.compile(optimizer, loss=sparse_cross_entropy)\n        # Train the model\n        self.decoder.fit([X_caption_embeddings,\n                          image_embeddings],\n                         Y_caption_embeddings,\n                         epochs=10,\n                         batch_size=16,\n                         shuffle=True,\n                         workers=16)\n    \n    # This function get the prediction by trained model\n    # Parameters:\n    #           X_image: Image path list\n    #           X_caption: Caption list\n    #           Y_caption: Label caption list for compare with the predict caption\n    def predict(self, X_image, X_caption, Y_caption):\n        \n        # Reshape of the data\n        image_embeddings, X_caption_embeddings, Y_caption_embeddings = self.preprocess_data(\n            X_image,\n            X_caption,\n            Y_caption,\n            test=True)\n        \n        images_list = []\n        captions_list = []\n        for example in range(0,len(image_embeddings)):\n            tokens = []\n            input_token = tokenizer.word_index['smark']\n            hidden_state = np.array(image_embeddings[example])\n            hidden_state = hidden_state.reshape((1, 1, 1000))\n            while input_token != tokenizer.word_index['emark'] and len(tokens)<self.num_result_words:\n                tokens.append(input_token)\n                tokens_array = self.embedding.predict(np.array(tokens))\n                tokens_array = tokens_array.reshape((1, -1, self.embedding_dimension))\n                token = self.decoder.predict([tokens_array, hidden_state],\n                                            workers=16)[0,-1,:]\n                input_token = np.argmax(token)\n            images_list.append(X_image[example])\n            captions_list.append(tokenizer.sequences_to_texts([tokens]))\n            if example%100==0:\n                print(\"%d images predicted\"%example)\n        \n        return images_list,captions_list\n    \n    \n    def save(self,model):\n        self.decoder.save_weights(model)\n    \n    def load(self,model):\n        self.decoder.load_weights(model)"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": "def sparse_cross_entropy(y_true, y_pred):\n    # Reshape and cast label caption\n    y_true = tf.reshape(y_true, [-1,15])\n    y_true = tf.dtypes.cast(y_true, tf.int32)\n    # Use tensorflow sparse softmax cross entropy loss function\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n                                                          logits=y_pred)\n    \n    # Get the scalar of the batch mean loss\n    loss_mean = tf.reduce_mean(loss)\n    return loss_mean"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": "model = CNN_GRU_Model()"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "('Procesando imagen', 0)\n('Procesando imagen', 1000)\n('Procesando imagen', 2000)\n('Procesando imagen', 3000)\n('Procesando imagen', 4000)\n('Procesando imagen', 5000)\n('Procesando imagen', 6000)\n('Procesando imagen', 7000)\n('Procesando imagen', 8000)\n('Procesando imagen', 9000)\n('Procesando imagen', 10000)\n('Procesando imagen', 11000)\n('Procesando imagen', 12000)\n('Procesando imagen', 13000)\n('Procesando imagen', 14000)\n('Procesando imagen', 15000)\n('Procesando imagen', 16000)\n('Procesando imagen', 17000)\n('Procesando imagen', 18000)\n('Procesando imagen', 19000)\n('Procesando imagen', 20000)\n('Procesando imagen', 21000)\n('Procesando imagen', 22000)\n('Procesando imagen', 23000)\n('Procesando imagen', 24000)\n('Procesando imagen', 25000)\n('Procesando imagen', 26000)\n('Procesando imagen', 27000)\n('Procesando imagen', 28000)\n('Procesando imagen', 29000)\n('Procesando imagen', 30000)\n('Procesando imagen', 31000)\n('Procesando imagen', 32000)\n('Procesando imagen', 33000)\n('Procesando imagen', 34000)\n('Procesando imagen', 35000)\n('Procesando imagen', 36000)\n('Procesando imagen', 37000)\n('Procesando imagen', 38000)\n('Procesando imagen', 39000)\n('Procesando imagen', 40000)\n('Procesando imagen', 41000)\n('Procesando imagen', 42000)\n('Procesando imagen', 43000)\n('Procesando imagen', 44000)\n('Procesando imagen', 45000)\n('Procesando imagen', 46000)\n('Procesando imagen', 47000)\n('Procesando imagen', 48000)\n('Procesando imagen', 49000)\n('Procesando imagen', 50000)\n('Procesando imagen', 51000)\n('Procesando imagen', 52000)\n('Procesando imagen', 53000)\n('Procesando imagen', 54000)\n('Procesando imagen', 55000)\n('Procesando imagen', 56000)\n('Procesando imagen', 57000)\n('Procesando imagen', 58000)\n('Procesando imagen', 59000)\n('Procesando imagen', 60000)\n('Procesando imagen', 61000)\n('Procesando imagen', 62000)\n('Procesando imagen', 63000)\n('Procesando imagen', 64000)\n('Procesando imagen', 65000)\n('Procesando imagen', 66000)\n('Procesando imagen', 67000)\n('Procesando imagen', 68000)\n('Procesando imagen', 69000)\n('Procesando imagen', 70000)\n('Procesando imagen', 71000)\n('Procesando imagen', 72000)\n('Procesando imagen', 73000)\n('Procesando imagen', 74000)\n('Procesando imagen', 75000)\n('Procesando imagen', 76000)\n('Procesando imagen', 77000)\n('Procesando imagen', 78000)\n('Procesando imagen', 79000)\n('Procesando imagen', 80000)\n('Procesando imagen', 81000)\n('Procesando imagen', 82000)\n('Procesando imagen', 83000)\n('Procesando imagen', 84000)\n('Procesando imagen', 85000)\n('Procesando imagen', 86000)\n('Procesando imagen', 87000)\n('Procesando imagen', 88000)\n('Procesando imagen', 89000)\n('Procesando imagen', 90000)\n('Procesando imagen', 91000)\n('Procesando imagen', 92000)\n('Procesando imagen', 93000)\n('Procesando imagen', 94000)\n('Procesando imagen', 95000)\n('Procesando imagen', 96000)\n('Procesando imagen', 97000)\n('Procesando imagen', 98000)\n('Procesando imagen', 99000)\n('Image embeddings:', (100000, 1, 1000))\n('X_caption_embeddings', (100000, 15, 512))\n('Y_caption_embeddings', (100000, 15))\nTrain on 100000 samples\nEpoch 1/10\n100000/100000 [==============================] - 59s 594us/sample - loss: 2.1560\nEpoch 2/10\n100000/100000 [==============================] - 59s 592us/sample - loss: 1.6694\nEpoch 3/10\n100000/100000 [==============================] - 59s 594us/sample - loss: 1.5314\nEpoch 4/10\n100000/100000 [==============================] - 59s 592us/sample - loss: 1.4444\nEpoch 5/10\n100000/100000 [==============================] - 59s 593us/sample - loss: 1.3836\nEpoch 6/10\n100000/100000 [==============================] - 59s 591us/sample - loss: 1.3400\nEpoch 7/10\n100000/100000 [==============================] - 59s 594us/sample - loss: 1.3051\nEpoch 8/10\n100000/100000 [==============================] - 59s 591us/sample - loss: 1.2772\nEpoch 9/10\n100000/100000 [==============================] - 59s 594us/sample - loss: 1.2532\nEpoch 10/10\n100000/100000 [==============================] - 58s 583us/sample - loss: 1.2332\n"
    }
   ],
   "source": "model.train(df['id_image'], pad_sentences_x, pad_sentences_y)"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": "PATH = \"data/validation.xlsx\"\ndf_test = pd.read_excel(PATH, names=[\"id_image\",\"caption\"])\ndf_test['caption'] = df_test.apply(lambda x: \"smark \"+x['caption']+\" emark\", axis=1)\ndf_test['id_image'] = df_test.apply(lambda x: idToPath(x['id_image']),axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": "test_sentences_x = tokenizer.texts_to_sequences(df_test['caption'])"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": "test_sentences_y = [sentence[1:] for sentence in test_sentences_x]"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": "test_pad_sentences_x = pad_sequences(test_sentences_x, padding='post',maxlen=15)\ntest_pad_sentences_y = pad_sequences(test_sentences_y, padding='post',maxlen=15)"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "('Procesando imagen', 0)\n('Procesando imagen', 1000)\n('Procesando imagen', 2000)\n('Procesando imagen', 3000)\n('Procesando imagen', 4000)\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-bd545749d236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pad_sentences_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pad_sentences_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-a486da7384bd>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X_image, X_caption, Y_caption)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mX_caption\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mY_caption\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             test=True)\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mimages_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-a486da7384bd>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(self, X_image, X_caption, Y_caption, test)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_caption\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_caption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Get the image embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mimage_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Lambda function for map the word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-a486da7384bd>\u001b[0m in \u001b[0;36mencoder_predict\u001b[0;34m(self, image_paths, load, test)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# Add the result of the prediction to the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Store the image embeddings list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.pyc\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.pyc\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.pyc\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/eager/def_function.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/eager/def_function.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/eager/function.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/eager/function.pyc\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/eager/function.pyc\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/eager/function.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/eager/execute.pyc\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": "img, captions = model.predict(df_test['id_image'], test_pad_sentences_x, test_pad_sentences_y)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "for i in range(len(img)):\n    print(img[i])\n    print(captions[i])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "result_df = pd.DataFrame(columns=[\"id_image\",\"caption\"])\nfor i in range(len(img)):\n    image = img[i][:-4].split('/')[-1]\n    caption = captions[i][0][6:]\n    result_df.loc[i] = [image,caption]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "result_df.to_csv(\"results/val_M041_E.csv\", encoding = 'utf-8',index=False)"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": "#model.save('./models/model2_pretranslate')"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": "model.load('./models/model2_pretranslate')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
