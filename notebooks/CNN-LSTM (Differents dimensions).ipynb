{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# CNN-LSTM model\n\nThis notebook aims the construction, training and test of a CNN-LSTM model purposed in Vinyals et\nal. [15].\n\nThe CNN-LSTM model is a encoder-decoder designed for image caption generation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Setup of libraries"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "In the next cell we link the GPU hardware to tensorflow for use this component in the training process."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": "import csv # To read the captions of a csv\nimport tensorflow as tf # To build and train the ANN\nfrom tensorflow.keras.preprocessing.text import Tokenizer # To use the tokenizer to split the words\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences # \nimport numpy as np\nimport pickle\nimport pandas as pd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Data reading"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "First, we upload the descriptions of a CSV to a list and the image id to another list to build the path where the images are stored."
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": "def idToPath(id_image):\n    id_image = str(id_image)\n    complete_name=id_image+\".jpg\"\n    while len(complete_name)<16:\n        complete_name=\"0\"+complete_name\n    return \"data/train2017/\"+complete_name"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": "PATH = \"data/train_human_spanish.xlsx\"\ndf = pd.read_excel(PATH, names=[\"id_image\",\"caption\"])\ndf['caption'] = df.apply(lambda x: \"smark \"+x['caption']+\" emark\", axis=1)\ndf['id_image'] = df.apply(lambda x: idToPath(x['id_image']),axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": "PATH = \"data/validation.xlsx\"\nval_df = pd.read_excel(PATH, names=[\"id_image\",\"caption\"])\nval_df['caption'] = val_df.apply(lambda x: \"smark \"+x['caption']+\" emark\", axis=1)\nval_df['id_image'] = val_df.apply(lambda x: idToPath(x['id_image']),axis=1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Setup of text data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We load the tokenizer that we initialized in word embeddings notebook."
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": "# loading\nwith open('items/tokenizer_spanish.pkl', 'rb') as handle:\n    tokenizer = pickle.load(handle)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We transform the descriptions into a list of integers that represent the index in the tokenizer's word index vocabulary."
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": "sentences_x = tokenizer.texts_to_sequences(df['caption'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "And its equivalent for the labels that are the same lists with each element moved one position to the left"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": "sentences_y = [sentence[1:] for sentence in sentences_x]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We put pads on the right for every sentence that has less than 15 words."
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": "pad_sentences_x = pad_sequences(sentences_x, padding='post',maxlen=15)\npad_sentences_y = pad_sequences(sentences_y, padding='post',maxlen=15)"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([   2,   15,  549,  568,    9,    4,  384,    7, 1980,  342,    3,\n          0,    0,    0,    0], dtype=int32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "pad_sentences_x[0]"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([  15,  549,  568,    9,    4,  384,    7, 1980,  342,    3,    0,\n          0,    0,    0,    0], dtype=int32)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "pad_sentences_y[0]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Model building"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The embedding layer loaded from the model stored in word embeddings notebook. This model transform word indexes to embeddings for encoder inputs."
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": "class Embeddings_Model(tf.keras.Model):\n    \n    def __init__(self, max_length, embedding_dimension):\n        super(Embeddings_Model, self).__init__()\n        weights = None\n        # Load the weight of the layer embedding pre-trained\n        with open('items/embeddingLayerWeights_spanish.pkl', 'rb') as handle:\n            weights = pickle.load(handle)\n        self.embedding = tf.keras.layers.Embedding(max_length+1, embedding_dimension ,weights=[weights])\n        \n    def call(self, inputs):\n        x = self.embedding(inputs)\n        \n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We define the enconder that is the InceptionV3 model. Its inputs are the images and the output is the image embedding."
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": "class CNN_Model(tf.keras.Model):\n    \n    def __init__(self):\n        super(CNN_Model, self).__init__()\n        # Load the InceptionV3 pre-trained model\n        self.input_model = tf.keras.applications.InceptionV3(include_top=True, weights='imagenet',classes=1000)\n        \n    def call(self, inputs):\n        x = self.input_model(inputs)\n        \n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We define the decoder that is composed by:\n\n- Embedding model: Transform word indexes to embeddings\n- LSTM module: Generate step by step the words.\n- Dense layer: Transform LSTM outputs to one hot vector (without softmax)"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": "class LSTM_Model(tf.keras.Model):\n    \n    def __init__(self, max_length, embedding_dimension, num_result_words):\n        super(LSTM_Model, self).__init__()\n        # Dimension embedding number\n        self.embedding_dimension = embedding_dimension\n        # Layer to map from image embedding to lstm dimension\n        self.dense = tf.keras.layers.Dense(embedding_dimension)\n        # LSTM layer with 0.1 dropout\n        self.lstm = tf.keras.layers.LSTM(embedding_dimension, input_shape=(num_result_words, embedding_dimension),\n                                         return_sequences=True, dropout=0.1)\n        # The dense output layer is similar to the softmax size but with a linear function\n        # because the softmax function is used in the loss function\n        self.output_model = tf.keras.layers.Dense(max_length, activation='linear')\n        \n    def call(self, inputs):\n        # input = (captions, initial_state)\n        captions, initial_state = inputs[0],inputs[1]\n        initial_state = self.dense(initial_state)\n        initial_state = tf.reshape(initial_state,[-1,self.embedding_dimension])\n        x = self.lstm(captions, initial_state=[initial_state, initial_state])\n        x = self.output_model(x)\n        \n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We combine the encoder and decoder models in a unique model."
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": "class CNN_LSTM_Model():\n    \n    # This function declare every layer of the model\n    def __init__(self):        \n        # Number of differents words in the tokenizer vocabulary.\n        self.max_length = 14276\n        # Embedding dimension\n        self.embedding_dimension = 512\n        # Maximum number or words that the model is able to generate in a caption.\n        self.num_result_words = 15\n        # Optimizer used\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n        # Initialize the encoder\n        self.encoder = CNN_Model()\n        # Initialize the embedding layer\n        self.embedding = Embeddings_Model(self.max_length, self.embedding_dimension)\n        # Initialize the decoder\n        self.decoder = LSTM_Model(self.max_length, self.embedding_dimension, self.num_result_words)\n    \n    # This function load all images for the encoder can transform them to image embeddings\n    # Parameters:\n    #            image_paths: Collection of path to images\n    #            load: Boolean that indicate if load all images or read a file with the embeddings\n    # Return:\n    #        A list with the image embeddings\n    def encoder_predict(self, image_paths, load=False, test=False):\n        predictions = []\n        contador=0\n        if load or test:\n            # If load=True then we need to predict all image embeddings with the encoder\n            # and stored for future uses\n            for image_path in image_paths:\n                # Visual indicator for longs process\n                if contador%1000==0:\n                    print(\"Procesando imagen\",contador)\n                contador+=1\n                # Read the file with tensorflow function\n                image = tf.io.read_file(image_path)\n                # Transform the image to jpeg in RGB color space\n                image = tf.image.decode_jpeg(image, channels=3)\n                # Resize to inception_v3 input size\n                image = tf.image.resize(image, (299, 299))\n                # Normalize and other transforms\n                image = tf.keras.applications.inception_v3.preprocess_input(image)\n                # Add a dimension to tensor for the model\n                image = np.expand_dims(image, axis=0)\n                # Add the result of the prediction to the list\n                predictions.append(self.encoder.predict(image))\n            # Store the image embeddings list\n            if not test and len(predictions)>10000:\n                with open('items/image_embeddings_list.pkl', 'wb') as handle:\n                    pickle.dump(predictions, handle, protocol=pickle.HIGHEST_PROTOCOL)\n        else:\n            # Load the image embeddings list\n            with open('items/image_embeddings_list.pkl', 'rb') as handle:\n                predictions = pickle.load(handle)[:len(image_paths)]\n        \n        return predictions\n    \n    # This function preprocess the data\n    # Parameters:\n    #           X_image: Image path list\n    #           X_caption: Caption list\n    #           Y_caption: Label caption list for compare with the predict caption\n    def preprocess_data(self,X_image,X_caption,Y_caption, test=False):\n        # Get the image embeddings\n        image_embeddings = np.array(self.encoder_predict(X_image, test))\n        \n        # Lambda function for map the word embeddings\n        map_embeddings = lambda x: self.embedding.predict(np.array([x]))\n        # Map the word embeddings\n        X_caption_embeddings = map_embeddings(X_caption)\n        \n        # A set of transforms in numpy array sizes\n        Y_caption_embeddings = np.array(Y_caption)\n        X_caption_embeddings = X_caption_embeddings.reshape((\n            -1, self.num_result_words, self.embedding_dimension))\n        Y_caption_embeddings = Y_caption_embeddings.reshape((\n            -1, self.num_result_words))\n        \n        return image_embeddings, X_caption_embeddings, Y_caption_embeddings\n    \n    # This function train the model\n    # Parameters:\n    #           X_image: Image path list\n    #           X_caption: Caption list\n    #           Y_caption: Label caption list for compare with the predict caption\n    def train(self, X_image, X_caption, Y_caption):\n        \n        # Reshape of the data\n        image_embeddings, X_caption_embeddings, Y_caption_embeddings = self.preprocess_data(\n            X_image,\n            X_caption,\n            Y_caption,\n            False)\n        \n        # Compile the model with sparse_cross_entropy as loss function\n        self.decoder.compile(self.optimizer, loss=sparse_cross_entropy)\n        # Train the model\n        self.decoder.fit([X_caption_embeddings,\n                          image_embeddings],\n                         Y_caption_embeddings,\n                         epochs=10,\n                         batch_size=16,\n                         shuffle=True,\n                         workers=16)\n    \n    # This function get the prediction by trained model\n    # Parameters:\n    #           X_image: Image path list\n    #           X_caption: Caption list\n    #           Y_caption: Label caption list for compare with the predict caption\n    def predict(self, X_image, X_caption, Y_caption):\n        \n        # Reshape of the data\n        image_embeddings, X_caption_embeddings, Y_caption_embeddings = self.preprocess_data(\n            X_image,\n            X_caption,\n            Y_caption,\n            test=True)\n        \n        images_list = []\n        captions_list = []\n        for example in range(0,len(image_embeddings)):\n            tokens = []\n            input_token = tokenizer.word_index['smark']\n            hidden_state = np.array(image_embeddings[example])\n            hidden_state = hidden_state.reshape((1, 1, 1000))\n            while input_token != tokenizer.word_index['emark'] and len(tokens)<self.num_result_words:\n                tokens.append(input_token)\n                tokens_array = self.embedding.predict(np.array(tokens))\n                tokens_array = tokens_array.reshape((1, -1, self.embedding_dimension))\n                token = self.decoder.predict([tokens_array, hidden_state],\n                                            workers=16)[0,-1,:]\n                input_token = np.argmax(token)\n            images_list.append(X_image[example])\n            captions_list.append(tokenizer.sequences_to_texts([tokens]))\n            if example%100==0:\n                print(\"%d images predicted\"%example)\n        \n        return images_list,captions_list\n    \n    \n    def save(self,model):\n        self.decoder.save_weights(model)\n    \n    def load(self,model):\n        self.decoder.load_weights(model)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We use a own loss function that get integer labels and logits predictions, this option is more efficient that transform label captions to logits"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": "def sparse_cross_entropy(y_true, y_pred):\n    # Reshape and cast label caption\n    y_true = tf.reshape(y_true, [-1,15])\n    y_true = tf.dtypes.cast(y_true, tf.int32)\n    # Use tensorflow sparse softmax cross entropy loss function\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n                                                          logits=y_pred)\n    \n    # Get the scalar of the batch mean loss\n    loss_mean = tf.reduce_mean(loss)\n    return loss_mean"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Initialize the Encoder-Decoder model"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n96116736/96112376 [==============================] - 5s 0us/step\n96124928/96112376 [==============================] - 5s 0us/step\n"
    }
   ],
   "source": "model = CNN_LSTM_Model()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Train with english human captions"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "('Image embeddings:', (9991, 1, 1000))\n('X_caption_embeddings', (9991, 15, 512))\n('Y_caption_embeddings', (9991, 15))\nTrain on 9991 samples\nEpoch 1/10\n3664/9991 [==========>...................] - ETA: 6s - loss: 3.8635"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2f3c558ed018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_sentences_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_sentences_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-10de5a3de444>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_image, X_caption, Y_caption)\u001b[0m\n\u001b[1;32m    114\u001b[0m                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                          \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                          workers=16)\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;31m# This function get the prediction by trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.pyc\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0mcurrent_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     with training_context.on_batch(\n\u001b[0;32m--> 126\u001b[0;31m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0m\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.pyc\u001b[0m in \u001b[0;36mon_batch\u001b[0;34m(self, step, mode, size)\u001b[0m\n\u001b[1;32m    779\u001b[0m       \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m       self.callbacks._call_batch_hook(\n\u001b[0;32m--> 781\u001b[0;31m           mode, 'begin', step, batch_logs)\n\u001b[0m\u001b[1;32m    782\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/callbacks.pyc\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     if (self._delta_t_batch > 0. and\n\u001b[1;32m    244\u001b[0m         delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3495\u001b[0m     \"\"\"\n\u001b[1;32m   3496\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3497\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3403\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3549\u001b[0m         \u001b[0;31m# warn and return nans like mean would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3550\u001b[0m         \u001b[0mrout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3551\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_median_nancheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3552\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m         \u001b[0;31m# if there are no nans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/utils.pyc\u001b[0m in \u001b[0;36m_median_nancheck\u001b[0;34m(data, result, axis, out)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m             warnings.warn(\"Invalid value encountered in median\",\n\u001b[1;32m   1146\u001b[0m                           RuntimeWarning, stacklevel=3)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": "model.train(df['id_image'][:-8], pad_sentences_x[:-8], pad_sentences_y[:-8])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "PATH = \"data/validation.xlsx\"\ndf_test = pd.read_excel(PATH, names=[\"id_image\",\"caption\"])\ndf_test['caption'] = df_test.apply(lambda x: \"smark \"+x['caption']+\" emark\", axis=1)\ndf_test['id_image'] = df_test.apply(lambda x: idToPath(x['id_image']),axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "test_sentences_x = tokenizer.texts_to_sequences(df_test['caption'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "test_sentences_y = [sentence[1:] for sentence in test_sentences_x]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "test_pad_sentences_x = pad_sequences(test_sentences_x, padding='post',maxlen=15)\ntest_pad_sentences_y = pad_sequences(test_sentences_y, padding='post',maxlen=15)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "img, captions = model.predict(df_test['id_image'], test_pad_sentences_x, test_pad_sentences_y)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "for i in range(len(img)):\n    print(img[i])\n    print(captions[i])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "self.embedding.predict(np.array([1]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#contador = len(path_dataset)-len(predictions)\n#print(contador)\nfor caption in predictions:\n    index_list = []\n    for token in caption:\n        index_list.append(np.argmax(token))\n    tokens_list = tokenizer.sequences_to_texts([index_list])\n    print(path_dataset[contador])\n    print(tokens_list)\n    contador+=1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "result_df = pd.DataFrame(columns=[\"id_image\",\"caption\"])\nfor i in range(len(img)):\n    image = img[i][:-4].split('/')[-1]\n    caption = captions[i][0][6:]\n    result_df.loc[i] = [image,caption]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "result_df.to_csv(\"results/val_M019_I.csv\", encoding = 'utf-8',index=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#model.save('./models/model1_pretranslate')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model.load('./models/model1_pretranslate')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
