{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Words embeddings\n\nThis notebooks aims create the word embeddings from the captions. The algorithm we use is called **Word2Vec**.\n\nWord2Vec, specifically the **skip-gram** algorithm uses an artificial neural network with a single hidden layer to predict the context of an input word.\n\nAfter training the artificial neural network, we use the hidden layer weights as an embedding matrix that transforms each input word (one hot vector format) into the embedding format. An example is the following image:\n\nBut this implementation is not very efficient, the calculation required to compute a large number of categories with softmax is expensive, and the models of a hidden layer often suffer from underfitting. For this reason, the skip-gram algorithm is somewhat different, the input of the model is a pair (word, context word) and the output is 1 if the peer is true otherwise is 0. Each element of the input pair have its own embedding matrix, but we only take the word matrix."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import csv # To read the captions of a csv\nimport tensorflow as tf # To build and train the ANN\nfrom tensorflow.keras.preprocessing.text import Tokenizer # To use the tokenizer to split the words\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences # \nimport numpy as np\nimport pandas as pd\nimport itertools"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "tf.config.set_visible_devices([], 'GPU')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "First, we upload the descriptions of a CSV to a list."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "PATH = \"data/train_machine_spanish.xlsx\"\ndf = pd.read_excel(PATH, names=[\"id_image\",\"caption\"])\ndf['caption'] = df.apply(lambda x: \"smark \"+x['caption']+\" emark\", axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "PATH = \"data/train_human_spanish.xlsx\"\ndf1 = pd.read_excel(PATH, names=[\"id_image\",\"caption\"])\ndf1['caption'] = df1.apply(lambda x: \"smark \"+x['caption']+\" emark\", axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "PATH = \"data/train_human_english.xlsx\"\ndf = pd.read_excel(PATH, names=[\"id_image\",\"caption\"])\ndf['caption'] = df.apply(lambda x: \"smark \"+x['caption']+\" emark\", axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df = pd.concat([df,df1])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We set some hyperparameters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# This variable adjust the dimensions of the embeddings. A high value may represent more complex embeddings\n# but the artificial neural network will be larger.\nembedding_dimension=512"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We create the tokenizer which is a usual tool in PLN that split sentences into tokens, where each token is a word."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['caption']) # The tokenizer build the word index from the captions\nword_index = tokenizer.word_index\nprint(\"Number of different words: %d\"%len(word_index))\n# This variable adjusts the maximum length of the vocabulary\n# If the number is less than the actual length, we remove the least used words\nmax_length = len(word_index)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We transform each description into an ordered list where each token is represented by its index in the tokenizer's vocabulary.\n\nWe don't use one hot encoding because this version of the algorithm is more efficiently with scalars."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sequences = tokenizer.texts_to_sequences(df['caption'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We create a set of pairs (word, context word) with his label 1 or 0 in case that the pair is true or not that we use for train the model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "peer_skipgrams = []\nlabel_skipgrams = []\ncount = 0\nfor sequence in sequences:\n    if count%1000==0:\n        print(\"%d sequences processed\"%count)\n    count+=1\n    ps, ls = tf.keras.preprocessing.sequence.skipgrams(\n        sequence, vocabulary_size=len(word_index), window_size=10, negative_samples=1.0, shuffle=True,\n        categorical=False, sampling_table=None, seed=None\n    )\n    peer_skipgrams[0:0] = ps\n    label_skipgrams[0:0] = ls\nprint(\"Number of pairs: %d\"%len(peer_skipgrams))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Split the pairs into two lists"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_list(tuples):\n    list1 = []\n    list2 = []\n    for i in tuples:\n        list1.append(i[0])\n        list2.append(i[1])\n    return list1, list2\ntrain_word, train_context = get_list(peer_skipgrams)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We build the branch that transforms the current word into embedding."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "word_input = tf.keras.layers.Input(shape=(1,))\nword_embedding = tf.keras.layers.Embedding(max_length+1, embedding_dimension, input_length=1)(word_input)\nword_reshape = tf.keras.layers.Reshape((embedding_dimension, ))(word_embedding)\n\nword_model = tf.keras.Model(word_input,word_reshape)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We build the branch that transforms the context word into embedding."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "context_input = tf.keras.layers.Input(shape=(1,))\ncontext_embedding = tf.keras.layers.Embedding(max_length+1, embedding_dimension, input_length=1)(context_input)\ncontext_reshape = tf.keras.layers.Reshape((embedding_dimension, ))(context_embedding)\n\ncontext_model = tf.keras.Model(context_input,context_reshape)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We build the merge of the two branchs and the output of the model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_input = tf.keras.layers.dot([word_reshape, context_reshape], axes=1, normalize=False)\nmodel_output = tf.keras.layers.Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid')(model_input)\nmodel = tf.keras.Model([word_input, context_input], model_output)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model.summary()\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We compute the separation threshold between train set and validation set."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train_val_test = int(len(train_word)*0.9)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We train the model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "n_epochs = 1\n\nhistory = model.fit([np.array(train_word[:train_val_test]),np.array(train_context[:train_val_test])],\n                    np.array(label_skipgrams[:train_val_test]) ,\n                    epochs=n_epochs,\n                    validation_data=([np.array(train_word[train_val_test:]), np.array(train_context[train_val_test:])],\n                                     np.array(label_skipgrams[train_val_test:]))\n                   , verbose=1, batch_size=256)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We extract the weight matrix from current embedding word for transform new words to embeddings."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "merge_layer = model.layers[2]\nweights = merge_layer.get_weights()[0]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We check that all the words are correctly coded. If some of the words are empty we must to replace the empty world for \"errorWord\" label."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "for i in word_index.keys():\n    if len(i)==1:\n        print(i.decode(\"unicode_escape\"))\n        print(word_index.keys().index(i.decode(\"unicode_escape\")))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We create vecs.tsv and meta.tsv files for use the projector of tensorflow https://projector.tensorflow.org/"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import io\n\nout_v = io.open('vecs_train_human_spanish.tsv', 'w', encoding='utf-8')\nout_m = io.open('meta_train_human_spanish.tsv', 'w', encoding='utf-8')\nfor token in word_index:\n        vec = weights[word_index[token]] # skip 0, it's padding.\n        out_m.write(token.decode('utf-8') + \"\\n\")\n        out_v.write('\\t'.join([str(x).decode('utf-8') for x in vec]) + \"\\n\")\nout_v.close()\nout_m.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We store the word index for next use of this dict."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\n\n# saving\nwith open('items/tokenizer_english.pkl', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We store the embedding layer for future transforms of words to embeddings"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pickle\n\n# saving\nwith open('items/embeddingLayerWeights_english.pkl', 'wb') as handle:\n    pickle.dump(weights, handle, protocol=pickle.HIGHEST_PROTOCOL)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
